{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.02721376, -0.37880325,  0.01586404,  0.5594002 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.00815481, -0.7696241 ,  0.04419282,  1.1581035 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.01873977, -0.38095975,  0.08494686,  0.608828  ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.03790217, -0.77344984,  0.11566365,  1.248568  ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.07276814, -1.1663395 ,  0.17213756,  1.9081641 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.12335196, -1.5592449 ,  0.25527954,  2.598408  ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:1 Score:6.0\n",
      "(array([ 0.03188365, -0.34964645,  0.0417469 ,  0.5707575 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.01398416, -0.7411309 ,  0.07068794,  1.1852058 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.01177833, -0.353266  ,  0.11270189,  0.6539043 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.02204123,  0.03325534,  0.13375454,  0.14819738], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.02464618, -0.36026344,  0.1463166 ,  0.8123622 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.04299253, -0.75388145,  0.185509  ,  1.4861897 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.07708447, -1.1474751 ,  0.25184485,  2.181723  ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:2 Score:7.0\n",
      "(array([ 0.0339822 ,  0.35935834, -0.01122191, -0.5851633 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.04445728, -0.0303924 , -0.0288459 , -0.01067094], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.03934765, -0.41978455, -0.02360386,  0.55615073], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.01866061, -0.80951667,  0.00434525,  1.1300366 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.00981876, -0.41970804,  0.04372037,  0.5546311 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.02271745, -0.03092756,  0.06033374,  0.00113351], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.02007041,  0.35749066,  0.05491801, -0.54497194], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.0018846 ,  0.7462924 ,  0.02762141, -1.0983775 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.03186205,  1.136091  , -0.0219915 , -1.6731775 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.08121309,  1.5271225 , -0.09490759, -2.281963  ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.14621529,  1.9188917 , -0.19259283, -2.9344428 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.21910062,  1.5328019 , -0.30539784, -2.4991214 ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:3 Score:12.0\n",
      "(array([-0.03558241,  0.42247277, -0.02500726, -0.62764746], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.01477426,  0.813543  , -0.05612221, -1.2323457 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.01388032,  0.42523745, -0.09992433, -0.6915064 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.02701774,  0.03829051, -0.12239201, -0.17710958], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.02468581, -0.34799936, -0.1244423 ,  0.32504332], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.00690282, -0.7344385 , -0.1064207 ,  0.8293612 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.02634509, -1.121787  , -0.0680981 ,  1.3499652 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.0672984 , -0.7302353 , -0.02036322,  0.7314816 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.09259987, -0.33963528,  0.00291561,  0.13797513], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.10228368,  0.05048501,  0.00259938, -0.44467372], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.10416745, -0.33970487, -0.00931754,  0.13951811], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.12165539, -0.72972107,  0.00205776,  0.71986514], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.15474725, -1.120209  ,  0.03671896,  1.3109993 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.19566284, -0.7313402 ,  0.08353958,  0.7577546 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.2210389 , -0.3438613 ,  0.10854437,  0.2324649 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.23092501,  0.04288777,  0.11271159, -0.2790381 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.22534253,  0.4297017 ,  0.09644775, -0.79125166], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.20428096,  0.81734204,  0.05958068, -1.3184278 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.17550373,  0.42597017,  0.01305795, -0.7047841 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.15456615,  0.81604457, -0.0209043 , -1.2863368 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.12582137,  0.42672607, -0.06663647, -0.72258997], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.10483278,  0.81880426, -0.10179786, -1.3524787 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.06815577,  1.211407  , -0.16235134, -2.0050704 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.01577148,  1.6040959 , -0.24931912, -2.6908538 ], dtype=float32), 1.0, True, False, {})\n",
      "(array([ 0.0445408 ,  1.219935  , -0.35280544, -2.2969532 ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:4 Score:24.0\n",
      "(array([-0.00271565, -0.38151163, -0.03350285,  0.5953195 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.02186886, -0.77097094, -0.00405118,  1.1630796 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.04880421, -0.38094106,  0.036593  ,  0.58254766], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.06795415, -0.7722986 ,  0.06597455,  1.19398   ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.10276432, -1.1642994 ,  0.11998595,  1.8259193 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.14546421, -0.7777483 ,  0.1879602 ,  1.3327338 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.1727277 , -0.39367175,  0.2367002 ,  0.8858477 ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:5 Score:7.0\n",
      "(array([-0.04268825, -0.4283504 ,  0.05218637,  0.6020064 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.0637385, -0.8200915,  0.0824397,  1.2228177], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.10046379, -1.212406  ,  0.13769907,  1.8641878 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.1450926 , -0.8263549 ,  0.20732749,  1.3838087 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.17430636, -0.44291067,  0.25825292,  0.9516785 ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:6 Score:5.0\n",
      "(array([-0.02526146, -0.3496822 ,  0.0331092 ,  0.60719985], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.04316013, -0.7409553 ,  0.06345569,  1.2166804 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.06891336, -0.35286984,  0.10668004,  0.6806787 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.07915833,  0.03385211,  0.12876154,  0.17092593], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.08173839, -0.35956842,  0.14220597,  0.83253944], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.1000561 , -0.7531092 ,  0.18218379,  1.5043303 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.12633039, -0.36872643,  0.23774278,  1.0542011 ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:7 Score:7.0\n",
      "(array([-0.01614016,  0.35262358, -0.0292629 , -0.60621554], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.00187515,  0.74379927, -0.05954661, -1.2133687 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.03554388,  1.135665  , -0.114296  , -1.8417233 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.07709668,  0.748938  , -0.18286274, -1.3446223 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.11099198,  1.1426663 , -0.24352507, -2.0384984 ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:8 Score:5.0\n",
      "(array([-0.02650492, -0.38020524, -0.04352023,  0.57457566], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.04560284, -0.769362  , -0.01496398,  1.135679  ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.07247103, -0.37903222,  0.02451642,  0.5481051 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.09154148, -0.7700773 ,  0.05244673,  1.1520356 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.11845656, -0.3816581 ,  0.09301242,  0.60822105], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.1376487 , -0.77432144,  0.12375066,  1.2524868 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.1647548 , -0.38812876,  0.16882013,  0.75848824], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.18421987, -0.7821282 ,  0.20597348,  1.443485  ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.21166341, -0.3985928 ,  0.2592749 ,  1.010695  ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:9 Score:9.0\n",
      "(array([-0.02679087, -0.35860646, -0.02525459,  0.5569297 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.03722579,  0.03218139, -0.00898803, -0.04070338], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.03983837, -0.35779113, -0.00481949,  0.53870535], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.05024623,  0.03243463,  0.01084477, -0.04629102], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.04504954,  0.4223788 ,  0.0032083 , -0.6250713 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([-0.02425285,  0.8127022 , -0.02762797, -1.212331  ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.01216458,  1.2038798 , -0.08214543, -1.8219566 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.05643739,  0.81624216, -0.14970222, -1.3024265 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.08522829,  0.43088815, -0.19695279, -0.8273317 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.10640763,  0.8252377 , -0.23699804, -1.5262467 ], dtype=float32), 0.0, True, False, {})\n",
      "Episode:10 Score:10.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1])\n",
    "        n_state, reward, done, info,_ = env.step(action)\n",
    "        print(env.step(action))\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected flatten_2_input to have shape (1, 4) but got array with shape (1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8429/2623574605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m     inputs, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1201\u001b[0;31m         x, extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m   1202\u001b[0m     \u001b[0;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;31m# at this point.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2332\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2334\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2359\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2361\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    584\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_2_input to have shape (1, 4) but got array with shape (1, 2)"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
